{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0bb14a",
   "metadata": {},
   "source": [
    "## PDF to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c4696e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Baymax_KnowledgeBase/2022, CURRENT Medical Diagnosis and Treatment- Original Revised.txt\n",
      "✅ Saved Baymax_KnowledgeBase/Symptoms to diagnosis Revised.txt\n",
      "✅ Saved Baymax_KnowledgeBase/Where there is no Doctor - David Werner Revised.1.txt\n",
      "✅ Saved Baymax_KnowledgeBase/National-Formulary-of-India-2011 Revised.txt\n",
      "✅ Saved Baymax_KnowledgeBase/03_PHC_IPHS_Guidelines-2022 Revised.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "# Input and output folders\n",
    "pdf_folder = \"pdfs\"\n",
    "output_folder = \"Baymax_KnowledgeBase\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each PDF\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "        # Remove .pdf and save as .txt\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        txt_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "        print(f\"✅ Saved {txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97752d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Renamed: 2022, CURRENT Medical Diagnosis and Treatment- Original Revised.txt → cmdt.txt\n",
      "✅ Renamed: Symptoms to diagnosis Revised.txt → symptom_flow.txt\n",
      "✅ Renamed: Where there is no Doctor - David Werner Revised.1.txt → rural_care.txt\n",
      "✅ Renamed: National-Formulary-of-India-2011 Revised.txt → nfi.txt\n",
      "✅ Renamed: 03_PHC_IPHS_Guidelines-2022 Revised.txt → iphs.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "rename_map = {\n",
    "    \"2022, CURRENT Medical Diagnosis and Treatment- Original Revised.txt\": \"cmdt.txt\",\n",
    "    \"Symptoms to diagnosis Revised.txt\": \"symptom_flow.txt\",\n",
    "    \"Where there is no Doctor - David Werner Revised.1.txt\": \"rural_care.txt\",\n",
    "    \"National-Formulary-of-India-2011 Revised.txt\": \"nfi.txt\",\n",
    "    \"03_PHC_IPHS_Guidelines-2022 Revised.txt\": \"iphs.txt\",\n",
    "}\n",
    "\n",
    "base_path = \"Baymax_KnowledgeBase\"\n",
    "\n",
    "for old_name, new_name in rename_map.items():\n",
    "    old_path = os.path.join(base_path, old_name)\n",
    "    new_path = os.path.join(base_path, new_name)\n",
    "    if os.path.exists(old_path):\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"✅ Renamed: {old_name} → {new_name}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found: {old_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb795ad1",
   "metadata": {},
   "source": [
    "## Smart Chunking & Metadata Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ebc209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devayushrout/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef4c9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457f7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17f59d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_chunk(text, source, max_tokens=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = count_tokens(sentence)\n",
    "        if current_tokens + tokens > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"source\": source\n",
    "                    }\n",
    "                })\n",
    "            current_chunk = sentence\n",
    "            current_tokens = tokens\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "            current_tokens += tokens\n",
    "\n",
    "    # Add last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            \"text\": current_chunk.strip(),\n",
    "            \"metadata\": {\n",
    "                \"source\": source\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5cf4e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 7435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "knowledge_dir = \"Baymax_KnowledgeBase\"\n",
    "all_chunks = []\n",
    "\n",
    "for filename in tqdm(os.listdir(knowledge_dir)):\n",
    "    filepath = os.path.join(knowledge_dir, filename)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    source_name = filename.replace(\".txt\", \"\")\n",
    "    chunks = smart_chunk(raw_text, source=source_name)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0da43249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Section\\nObjectives of IPHS for HWC-PHC | 5 \\nObjectives of IPHS \\nfor HWC-PHC\\n3\\nThe broad objectives of the Indian Public Health Standards (IPHS) for PHC in rural and urban areas include \\nthe following:\\n1. To define uniform benchmark to ensure high quality services that are accountable, responsive, and \\nsensitive to the needs of the community. 2. To specify the minimum assured (essential) and achievable (desirable) services that are expected to \\nbe provided at different levels of public health facilities. 3. To provide guidance on health systems strengthening components which includes architectural \\ndesign of facilities, human resources for health, drugs, diagnostics, equipment, administrative and \\nlogistical support services to improve the overall health related outcomes \\n4. To achieve and maintain an acceptable standard of the quality of care at public facilities\\n5. To facilitate monitoring and supervision of the facilities\\n6. To provide guidance and tools for governance, leadership and evaluation. Section\\nPopulation Norms for HWC-PHC | 7 \\nPopulation Norms \\nfor HWC-PHC\\n5\\nNormally, a PHC in rural areas is to be established for a population of 20,000 (in hilly and tribal areas) and \\n30,000 (in plains). It should be established co-terminus with Panchayats (depending upon the population) to \\nestablish effective convergence and linkages with citizen centric services. A Primary Health Centre (PHC) that \\nis linked to a cluster of Sub Health Centre - HWCs would be strengthened as HWC to deliver the expanded \\nrange of primary care services with complete 12 package of services. In addition, it would also serve as the \\nfirst point of referral for all the SHC-HWCs in its jurisdiction. In urban areas, usually the population density is high and there are various types of health care facilities \\nwhich provide inpatient care. So, the approach in urban areas for establishing PHCs shall be different from \\nthat in rural areas. UPHCs are established for every 50,000 population, and in close proximity to urban \\nslums.', 'metadata': {'source': 'iphs'}}\n"
     ]
    }
   ],
   "source": [
    "print(all_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "046e1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"baymax_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa37c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "docs = []\n",
    "\n",
    "base_path = \"Baymax_KnowledgeBase\"\n",
    "\n",
    "# Manual source labels for each file\n",
    "file_to_source = {\n",
    "    \"rural_care.txt\": \"rural_care\",\n",
    "    \"cmdt.txt\": \"clinical_guidelines\",\n",
    "    \"symptom_flow.txt\": \"consultation_flow\",\n",
    "    \"iphs.txt\": \"protocol_guidelines\",\n",
    "    \"nfi.txt\": \"medication_safety\"\n",
    "}\n",
    "\n",
    "# Optional: Priority settings (used later for filtering)\n",
    "file_priority = {\n",
    "    \"rural_care.txt\": 1,\n",
    "    \"symptom_flow.txt\": 2,\n",
    "    \"iphs.txt\": 2,\n",
    "    \"cmdt.txt\": 3,\n",
    "    \"nfi.txt\": 3\n",
    "}\n",
    "\n",
    "# Loop and load\n",
    "for filename in os.listdir(base_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(base_path, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            docs.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": file_to_source.get(filename, \"unknown\"),\n",
    "                    \"priority\": file_priority.get(filename, 3),\n",
    "                    \"filename\": filename\n",
    "                }\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf326ff",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22572185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 7435 chunks into LangChain Document format.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "\n",
    "with open(\"baymax_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "document_chunks = [\n",
    "    Document(page_content=chunk[\"text\"], metadata=chunk[\"metadata\"])\n",
    "    for chunk in loaded_chunks\n",
    "]\n",
    "\n",
    "print(f\"✅ Loaded {len(document_chunks)} chunks into LangChain Document format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7ea4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f993df",
   "metadata": {},
   "source": [
    "## VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70d309ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorstore saved as 'baymax_vectorstore/'\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Build vectorstore\n",
    "vectorstore = FAISS.from_documents(document_chunks, embedding_model)\n",
    "\n",
    "# Save locally\n",
    "vectorstore.save_local(\"baymax_vectorstore\")\n",
    "print(\"✅ Vectorstore saved as 'baymax_vectorstore/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7769f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
