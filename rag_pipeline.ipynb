{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005de078",
   "metadata": {},
   "source": [
    "## Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0488205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devayushrout/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153f74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMPTOM_KEYWORDS = [\n",
    "    \"fever\", \"cough\", \"cold\", \"pain\", \"headache\", \"vomiting\", \"diarrhea\",\n",
    "    \"sore throat\", \"rash\", \"swelling\", \"chills\", \"fatigue\", \"nausea\",\n",
    "    \"shortness of breath\", \"wheezing\", \"bleeding\", \"dizziness\", \"burns\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b34b170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done. Total chunks created: 4599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "SYMPTOM_KEYWORDS = [\n",
    "    \"fever\", \"cough\", \"cold\", \"pain\", \"headache\", \"vomiting\", \"diarrhea\",\n",
    "    \"sore throat\", \"rash\", \"swelling\", \"chills\", \"fatigue\", \"nausea\",\n",
    "    \"shortness of breath\", \"wheezing\", \"bleeding\", \"dizziness\", \"burns\",\n",
    "]\n",
    "\n",
    "def find_symptoms(sentence):\n",
    "    symptoms = [s for s in SYMPTOM_KEYWORDS if s.lower() in sentence.lower()]\n",
    "    return symptoms\n",
    "\n",
    "def smart_chunk_text(text, source_name, priority=1, chunk_size=4):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        group = sentences[i:i+chunk_size]\n",
    "        combined = \" \".join(group)\n",
    "        found = []\n",
    "\n",
    "        for sent in group:\n",
    "            found += find_symptoms(sent)\n",
    "\n",
    "        found = list(set(found))  # Remove duplicates\n",
    "\n",
    "        if found:\n",
    "            chunk = {\n",
    "                \"text\": combined,\n",
    "                \"symptoms\": found,\n",
    "                \"source\": source_name,\n",
    "                \"type\": \"rural_remedy\" if \"no_doctor\" in source_name else \"clinical\",\n",
    "                \"priority\": priority\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        i += chunk_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ðŸ”„ Loop over all text files in your knowledge base\n",
    "all_chunks = []\n",
    "\n",
    "root_folder = \"Baymax_KnowledgeBase\"  # â† your base folder with 5 sources\n",
    "\n",
    "for folder in os.listdir(root_folder):\n",
    "    folder_path = os.path.join(root_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        priority = 1 if \"no_doctor\" in folder else 2 if \"iphs\" in folder else 3\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                    chunks = smart_chunk_text(text, folder, priority)\n",
    "                    all_chunks.extend(chunks)\n",
    "\n",
    "# ðŸ’¾ Save all chunks as a JSONL file\n",
    "with open(\"symptom_chunks.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Done. Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c408c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4599 chunks.\n",
      "âœ… FAISS vectorstore saved to 'baymax_vectorstore/'\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "# Load your symptom-tagged chunks\n",
    "chunks = []\n",
    "with open(\"symptom_chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        chunks.append(\n",
    "            Document(\n",
    "                page_content=data[\"text\"],\n",
    "                metadata={\n",
    "                    \"symptoms\": data[\"symptoms\"],\n",
    "                    \"source\": data[\"source\"],\n",
    "                    \"type\": data[\"type\"],\n",
    "                    \"priority\": data[\"priority\"]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks.\")\n",
    "\n",
    "# Set up embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS vectorstore from chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save it to disk\n",
    "vectorstore.save_local(\"baymax_vectorstore\")\n",
    "print(\"âœ… FAISS vectorstore saved to 'baymax_vectorstore/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3226c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Text: Where There Is No Doctor 2011\n",
      "162\n",
      "HEADACHES AND MIGRAINES\n",
      "SIMPLE HEADACHE can be helped by rest \n",
      "and aspirin. It often helps to put a cloth \n",
      "soaked in hot water on the back of the \n",
      "neck and to massage (rub) the neck \n",
      "and shoulders gently. Some other home \n",
      "remedies also seem to help. Headache is comm\n",
      "Metadata: {'symptoms': ['fever', 'headache'], 'source': 'rural_care', 'type': 'clinical', 'priority': 3}\n",
      "\n",
      "---\n",
      "Text: Let \n",
      "the air reach his body. This will help the fever go \n",
      "down (see p. 76). True. It helps.\n",
      "Metadata: {'symptoms': ['fever'], 'source': 'rural_care', 'type': 'clinical', 'priority': 3}\n",
      "\n",
      "---\n",
      "Text: 4. \u0007Pour cool (not cold) water over him, or put cloths soaked in cool water on his \n",
      "chest and forehead. Fan the cloths and change them often to keep them cool. Continue to do this until the fever goes down (below 38Â°).\n",
      "Metadata: {'symptoms': ['cold', 'fever'], 'source': 'rural_care', 'type': 'clinical', 'priority': 3}\n",
      "\n",
      "---\n",
      "Text: Bring the fever down as soon \n",
      "as you can and treat the cause of the fever, if possible. High fever can cause seizures \n",
      "(convulsions) and is most dangerous for small children. When a fever goes very high (over 40Â°), it must be lowered at once:\n",
      "1. Put the person in a cool place.\n",
      "Metadata: {'symptoms': ['fever'], 'source': 'rural_care', 'type': 'clinical', 'priority': 3}\n",
      "\n",
      "---\n",
      "Text: Ask a health worker which medicine works best where \n",
      "you live. â™¦\t Lower the fever with cool wet cloths (see p. 76). â™¦\t Give plenty of liquids: soups, juices, and Rehydration Drink to avoid dehydration \n",
      "(see p. 152). â™¦\t Give nutritious foods, in liquid form if necessary.\n",
      "Metadata: {'symptoms': ['fever'], 'source': 'rural_care', 'type': 'clinical', 'priority': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load saved vectorstore\n",
    "vs = FAISS.load_local(\"baymax_vectorstore\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Test a query\n",
    "docs = vs.similarity_search(\"What to do if someone has fever and headache?\", k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"\\n---\")\n",
    "    print(\"Text:\", doc.page_content[:300])\n",
    "    print(\"Metadata:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8408165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symptom_map.py\n",
    "symptom_synonyms = {\n",
    "    \"fever\": [\"bukhar\", \"tapman\", \"high temperature\"],\n",
    "    \"headache\": [\"sar dard\", \"sar mein dard\", \"migraine\"],\n",
    "    \"cough\": [\"khaansi\", \"khansi\", \"dry cough\"],\n",
    "    \"cold\": [\"zukaam\", \"runny nose\", \"nasal congestion\"],\n",
    "    \"vomiting\": [\"ulti\", \"throwing up\", \"nausea\"],\n",
    "    \"diarrhea\": [\"patla mal\", \"loose motions\"],\n",
    "    \"body pain\": [\"jodo ka dard\", \"sareer mein dard\", \"body ache\"],\n",
    "    \"sore throat\": [\"gale mein dard\", \"gala kharab\"],\n",
    "    # Keep adding more\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18709301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def normalize_symptoms(user_input):\n",
    "    normalized = set()\n",
    "\n",
    "    for standard, synonyms in symptom_synonyms.items():\n",
    "        for term in synonyms:\n",
    "            score = fuzz.partial_ratio(term.lower(), user_input.lower())\n",
    "            if score >= 85:\n",
    "                normalized.add(standard)\n",
    "    return list(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100f1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized symptoms: ['fever', 'headache']\n"
     ]
    }
   ],
   "source": [
    "query = \"Mujhe bukhar aur sar dard hai\"\n",
    "matched = normalize_symptoms(query)\n",
    "\n",
    "print(\"Normalized symptoms:\", matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0df9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_symptoms(symptom_list, vectorstore, top_k=3):\n",
    "    joined = \" \".join(symptom_list)\n",
    "    results = vectorstore.similarity_search(joined, k=top_k)\n",
    "\n",
    "    for doc in results:\n",
    "        print(\"\\n---\")\n",
    "        print(\"Text:\", doc.page_content[:300])\n",
    "        print(\"Metadata:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14dabd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_chunk_by_source(symptoms, vectorstore, sources=[\"rural_care\", \"cmdt\", \"nfi\"], k=5):\n",
    "    joined = \" \".join(symptoms)\n",
    "    results = vectorstore.similarity_search_with_score(joined, k=k)\n",
    "\n",
    "    selected = {}\n",
    "    for doc, score in results:\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        if source in sources and source not in selected:\n",
    "            selected[source] = doc\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2e91e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = ['fever', 'cough']\n",
    "chunks = get_top_chunk_by_source(symptoms, db)\n",
    "\n",
    "for source, doc in chunks.items():\n",
    "    print(f\"\\nðŸ“˜ {source} â†’ {doc.metadata}\")\n",
    "    print(doc.page_content[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e5017",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2 â€“ Load DB\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaymax_vectorstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 3 â€“ Search one best chunk per book\u001b[39;00m\n\u001b[1;32m     11\u001b[0m symptoms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfever\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcough\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Baymax/venv/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:1190\u001b[0m, in \u001b[0;36mFAISS.load_local\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load FAISS index, docstore, and index_to_docstore_id from disk.\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m        arbitrary code on your machine.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_deserialization:\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe de-serialization relies loading a pickle file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickle files can be modified to deliver a malicious payload that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults in execution of arbitrary code on your machine.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will need to set `allow_dangerous_deserialization` to `True` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable deserialization. If you do this, make sure that you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust the source of the data. For example, if you are loading a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile that you created, and know that no one else has modified the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, then this is safe to do. Do not set this to `True` if you are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading a file from an untrusted source (e.g., some random site on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe internet.).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(folder_path)\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;66;03m# load index separately since it is not picklable\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1 â€“ Load model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 â€“ Load DB\n",
    "db = FAISS.load_local(\"baymax_vectorstore\", embedding_model, allow_dangerous_deserialization=True)\n",
    "# Step 3 â€“ Search one best chunk per book\n",
    "symptoms = ['fever', 'cough']\n",
    "chunks = get_top_chunk_by_source(symptoms, db)\n",
    "\n",
    "# Step 4 â€“ Feed into your LLM\n",
    "print(chunks[\"rural_care\"].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
